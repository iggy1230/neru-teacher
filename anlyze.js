// --- anlyze.js (æŠœç²‹: startMicrophoneã®ã¿ä¿®æ­£) ---
// â€»ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã¯é•·ã„ã®ã§ã€è©²å½“é–¢æ•°ã ã‘æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚

async function startMicrophone() {
    try {
        // 1. Web Speech API (æ–‡å­—èµ·ã“ã—ç”¨)
        if ('webkitSpeechRecognition' in window) {
            recognition = new webkitSpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = false;
            recognition.lang = 'ja-JP';
            
            isRecognitionActive = true;

            recognition.onresult = (event) => {
                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        const transcript = event.results[i][0].transcript;
                        console.log("ðŸŽ¤ èªè­˜å®Œäº†:", transcript); // ãƒ–ãƒ©ã‚¦ã‚¶ãƒ­ã‚°
                        
                        if (liveSocket && liveSocket.readyState === WebSocket.OPEN) {
                            console.log("ðŸ“¤ ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡è©¦è¡Œ:", transcript); // â˜…é€ä¿¡ç¢ºèª
                            liveSocket.send(JSON.stringify({ type: 'log_text', text: transcript }));
                        } else {
                            console.warn("âš ï¸ ã‚½ã‚±ãƒƒãƒˆæœªæŽ¥ç¶šã®ãŸã‚é€ä¿¡ä¸å¯");
                        }
                    }
                }
            };
            
            recognition.onend = () => {
                if (isRecognitionActive) {
                    console.log("ðŸ”„ éŸ³å£°èªè­˜å†èµ·å‹•");
                    try { recognition.start(); } catch(e) {}
                }
            };
            
            recognition.start();
        } else {
             console.warn("ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°èªè­˜éžå¯¾å¿œã§ã™");
        }

        // 2. Audio Worklet (éŸ³å£°é…ä¿¡ç”¨)
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: { sampleRate: 16000, channelCount: 1 } });
        const processorCode = `class PcmProcessor extends AudioWorkletProcessor { constructor() { super(); this.bufferSize = 2048; this.buffer = new Float32Array(this.bufferSize); this.index = 0; } process(inputs, outputs, parameters) { const input = inputs[0]; if (input.length > 0) { const channel = input[0]; for (let i = 0; i < channel.length; i++) { this.buffer[this.index++] = channel[i]; if (this.index >= this.bufferSize) { this.port.postMessage(this.buffer); this.index = 0; } } } return true; } } registerProcessor('pcm-processor', PcmProcessor);`;
        const blob = new Blob([processorCode], { type: 'application/javascript' });
        await audioContext.audioWorklet.addModule(URL.createObjectURL(blob));
        const source = audioContext.createMediaStreamSource(mediaStream);
        workletNode = new AudioWorkletNode(audioContext, 'pcm-processor');
        source.connect(workletNode);
        
        workletNode.port.onmessage = (event) => {
            const inputData = event.data;
            let sum = 0; for(let i=0; i<inputData.length; i++) sum += inputData[i] * inputData[i];
            const volume = Math.sqrt(sum / inputData.length);
            
            const btn = document.getElementById('mic-btn');
            if (btn) btn.style.boxShadow = volume > 0.01 ? `0 0 ${10 + volume * 500}px #ffeb3b` : "none";
            
            // éŸ³å£°é€ä¿¡ (750msé…å»¶)
            setTimeout(() => {
                if (!liveSocket || liveSocket.readyState !== WebSocket.OPEN) return;
                const downsampled = downsampleBuffer(inputData, audioContext.sampleRate, 16000);
                const pcmBuffer = floatTo16BitPCM(downsampled);
                const base64Audio = arrayBufferToBase64(pcmBuffer);
                liveSocket.send(JSON.stringify({ base64Audio: base64Audio }));
            }, 750);
        };
    } catch(e) { updateNellMessage("ãƒžã‚¤ã‚¯ã‚¨ãƒ©ãƒ¼", "thinking"); }
}